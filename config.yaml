# config.yaml
num_epochs: 10
train_batch_size: 32
eval_batch_size: 64
learning_rate: 2e-5
optimizer: "AdamW"
optimizer_params:
  lr: 2e-5
  betas: [0.9, 0.999]
  weight_decay: 0.01
scheduler: "OneCycleLR"
scheduler_params:
  max_lr: 2e-5
  total_steps: 10000
  anneal_strategy: "cos"
  pct_start: 0.3
  div_factor: 25.0
  final_div_factor: 1e4
weight_decay: 0.01
max_grad_norm: 1.0
early_stopping_patience: 3
gradient_accumulation_steps: 1
mixed_precision: true
seed: 42

base_model_name: "bert-base-uncased"
num_labels: 2
classifier_dropout: 0.1
mora:
  num_levels: 2
  num_experts_per_level: 4
  expert_hidden_size: 256
  in_features: 768
  out_features: 768
  max_act_steps: 10

dataset_path: "./data/dataset.csv"
max_length: 128
val_size: 0.1
num_workers: 4

checkpoint_dir: "./checkpoints"
best_model_name: "best_model.pth"
checkpoint_interval: 500
save_every_epoch: true

logging_steps: 50
eval_steps: 200

use_sam: false
sam_params:
  rho: 0.05
  adaptive: false
